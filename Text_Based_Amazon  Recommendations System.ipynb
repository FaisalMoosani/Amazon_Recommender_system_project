{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import math\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "import plotly\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "\n",
    "plotly.offline.init_notebook_mode(connected = True)\n",
    "\n",
    "import itertools\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Importing and Initial analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"../data/tops_fashion.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['asin','brand','color','medium_image_url','product_type_name','title','formatted_price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_understanding(column):\n",
    "    print(\"**********High level description**********\\n\")\n",
    "    print(column.describe())\n",
    "    \n",
    "    print(\"\\n **********Unique values********** \\n\")\n",
    "    print(column.unique())\\\n",
    "    \n",
    "    print('\\n **********Top ten values********** \\n')\n",
    "    count = Counter(list(column))\n",
    "    print(count.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for column product type name\n",
    "data_understanding(data['product_type_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for column Brand\n",
    "data_understanding(data['brand'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for column Color\n",
    "data_understanding(data['color'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for column Price\n",
    "data_understanding(data['formatted_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for column title\n",
    "data_understanding(data['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing null value rows from color and price\n",
    "data = data.loc[~data['formatted_price'].isnull()]\n",
    "data = data.loc[~data['color'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check duplicate items\n",
    "\n",
    "print(sum(data.duplicated('title')))\n",
    "\n",
    "## Remove all products with very few words in title\n",
    "\n",
    "data_sorted = data[data['title'].apply(lambda x : len(x.split()) > 4 )]\n",
    "print(\"After removal of products with short description:\", data_sorted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sort data based on title (alphabatical order of title)\n",
    "\n",
    "data_sorted.sort_values('title', inplace = True, ascending = False)\n",
    "data_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "for i , row in data_sorted.iterrows():\n",
    "    indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1_dedup_asins = []\n",
    "i = 0\n",
    "j = 0\n",
    "num_data_points = data_sorted.shape[0]\n",
    "\n",
    "while i < num_data_points and j < num_data_points:\n",
    "    previous_i = i\n",
    "    \n",
    "    # store the list of words of ith string in a\n",
    "    a = data['title'].loc[indices[i]].split()\n",
    "    \n",
    "    j = i+1\n",
    "    \n",
    "    while j < num_data_points:\n",
    "        #store the list of words of jth string in b\n",
    "        b = data['title'].loc[indices[j]].split()\n",
    "        \n",
    "        # store the maximum length of two strings\n",
    "        length = max(len(a) , len(b))\n",
    "        \n",
    "        count = 0 # count is used to store the number of words that are matched in both strings\n",
    "        \n",
    "        for k in itertools.zip_longest(a,b): #it will give [('a','a'),('b','b'),('c','d'),('d','none')]\n",
    "            if (k[0] == k[1]):\n",
    "                count += 1\n",
    "                \n",
    "                \n",
    "        if (length - count) > 2:\n",
    "            stage1_dedup_asins.append(data_sorted['asin'].loc[indices[i]])\n",
    "            \n",
    "            \n",
    "            i = j\n",
    "            break\n",
    "        else:\n",
    "            j += 1\n",
    "    if previous_i == i:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data['asin'].isin(stage1_dedup_asins)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "for i,row in data.iterrows():\n",
    "    indices.append(i)\n",
    "\n",
    "stage2_dedupe_asins = []\n",
    "while len(indices)!=0:\n",
    "    i = indices.pop()\n",
    "    stage2_dedupe_asins.append(data['asin'].loc[i])\n",
    "    # consider the first apperal's title\n",
    "    a = data['title'].loc[i].split()\n",
    "    # store the list of words of ith string in a, ex: a = ['tokidoki', 'The', 'Queen', 'of', 'Diamonds', 'Women's', 'Shirt', 'X-Large']\n",
    "    for j in indices:\n",
    "        \n",
    "        b = data['title'].loc[j].split()\n",
    "        # store the list of words of jth string in b, ex: b = ['tokidoki', 'The', 'Queen', 'of', 'Diamonds', 'Women's', 'Shirt', 'X-Large']\n",
    "        \n",
    "        length = max(len(a),len(b))\n",
    "        \n",
    "        # count is used to store the number of words that are matched in both strings\n",
    "        count  = 0\n",
    "\n",
    "        for k in itertools.zip_longest(a,b): \n",
    "            if (k[0]==k[1]):\n",
    "                count += 1\n",
    "\n",
    "        # if the number of words in which both strings differ are < 3 , we are considering it as those two apperals are same, hence we are ignoring them\n",
    "        if (length - count) < 3:\n",
    "            indices.remove(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data['asin'].isin(stage2_dedupe_asins)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('../code/16k_apperal_data_preprocessed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to remove stop words and other pre-processing task\n",
    "\n",
    "def text_preprocessing(text, index, col):\n",
    "    if type(text) is not int:\n",
    "        string = \"\"\n",
    "        \n",
    "        for words in text.split():\n",
    "            #remove special characters\n",
    "            word = (\"\".join(e for e in words if e.isalnum()))\n",
    "            # convert to lower case\n",
    "            word = word.lower()\n",
    "            # stop words removal\n",
    "            if not words in stop_words:\n",
    "                string += word + \" \"\n",
    "        data[col][index] = string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index , row in data.iterrows():\n",
    "    text_preprocessing(row['title'], index, 'title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Based Similarity models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to display an image\n",
    "def display_img(url, ax, fig):\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting heatmaps for algo decisions\n",
    "def plot_heatmap(keys, values, labels, url, text):\n",
    "    # divide whole figure into two parts\n",
    "    gs = gridspec.GridSpec(2,2,width_ratios = [4,1], height_ratios = [4,1])\n",
    "    fig = plt.figure(figsize = (25,3))\n",
    "    \n",
    "    # plot heatmap of that represent count of commonly occured words in title\n",
    "    ax = plt.subplot(gs[0])\n",
    "    # display cell in white if words of title1 intersect with word of title2 , if not, black\n",
    "    ax = sns.heatmap(np.array([values]) , annot = np.array([labels]))\n",
    "    ax.set_xticklabels(keys) \n",
    "    ax.set_title(text)\n",
    "    \n",
    "    # plot image of the apparel\n",
    "    ax = plt.subplot(gs[1])\n",
    "    ax.grid(False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    # call display image\n",
    "    display_img(url, ax, fig)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap_image(doc_id, vec1, vec2, url, text, model):\n",
    "    # find the common words between titles\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    \n",
    "    #set values of non intersecting words to zero\n",
    "    for i in vec2:\n",
    "        if i not in intersection:\n",
    "            vec2[i] = 0\n",
    "    \n",
    "    # for labeling heatmap, keys contains list of all words in title2\n",
    "    keys = list(vec2.keys())\n",
    "        #  if ith word in intersection(lis of words of title1 and list of words of title2): values(i)=count of that word in title2 else values(i)=0 \n",
    "    values = [vec2[x] for x in vec2.keys()]\n",
    "    \n",
    "    # labels: len(labels) == len(keys), the values of labels depends on the model we are using\n",
    "        # if model == 'bag of words': labels(i) = values(i)\n",
    "        # if model == 'tfidf weighted bag of words':labels(i) = tfidf(keys(i))\n",
    "        # if model == 'idf weighted bag of words':labels(i) = idf(keys(i))\n",
    "\n",
    "    if model == 'bag_of_words':\n",
    "        labels = values\n",
    "    elif model == 'tfidf':\n",
    "        labels = []\n",
    "        for x in vec2.keys():\n",
    "            # tfidf_title_vectorizer.vocabulary_ it contains all the words in the corpus\n",
    "            # tfidf_title_features[doc_id, index_of_word_in_corpus] will give the tfidf value of word in given document (doc_id)\n",
    "            if x in  tfidf_title_vectorizer.vocabulary_:\n",
    "                labels.append(tfidf_title_features[doc_id, tfidf_title_vectorizer.vocabulary_[x]])\n",
    "            else:\n",
    "                labels.append(0)\n",
    "    elif model == 'idf':\n",
    "        labels = []\n",
    "        for x in vec2.keys():\n",
    "            # idf_title_vectorizer.vocabulary_ it contains all the words in the corpus\n",
    "            # idf_title_features[doc_id, index_of_word_in_corpus] will give the idf value of word in given document (doc_id)\n",
    "            if x in  idf_title_vectorizer.vocabulary_:\n",
    "                labels.append(idf_title_features[doc_id, idf_title_vectorizer.vocabulary_[x]])\n",
    "            else:\n",
    "                labels.append(0)\n",
    "\n",
    "    plot_heatmap(keys, values, labels, url, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get list of words along with frequency\n",
    "def text_to_vector(text):\n",
    "    word = re.compile(r'\\w+')\n",
    "    words = word.findall(text)\n",
    "    \n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(doc_id, content_a, content_b, url, model):\n",
    "    text1 = content_a\n",
    "    text2 = content_b\n",
    "    \n",
    "    vector1 = text_to_vector(text1)\n",
    "    vector2 = text_to_vector(text2)\n",
    "    \n",
    "    plot_heatmap_image(doc_id, vector1, vector2, url, text2, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bag Of Words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "title_features = vectorizer.fit_transform(data['title'])\n",
    "title_features.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_model(doc_id, num_results):\n",
    "    #doc_id = id of product\n",
    "    #num_results = how many similar products\n",
    "    \n",
    "    #distance between query product and all other products\n",
    "    pair_wise_dist = pairwise_distances(title_features, title_features[doc_id])\n",
    "    \n",
    "    #indices of smalles distances\n",
    "    indices = np.argsort(pair_wise_dist.flatten())[0:num_results]\n",
    "    \n",
    "    # smalles distances\n",
    "    pdists = np.sort(pair_wise_dist.flatten())[0:num_results]\n",
    "    \n",
    "    df_indices = list(data.index[indices])\n",
    "    \n",
    "    for i in range(0, len(indices)):\n",
    "        get_result(indices[i],data['title'].loc[df_indices[0]], data['title'].loc[df_indices[i]], data['medium_image_url'].loc[df_indices[i]], 'bag_of_words')\n",
    "        \n",
    "        print('ASIN:', data['asin'].loc[df_indices[i]])\n",
    "        print('Brand:', data['brand'].loc[df_indices[i]])\n",
    "        print('Title:', data['title'].loc[df_indices[i]])\n",
    "        print('Euclidean similarity with the query image :', pdists[i])\n",
    "        print('*'*60)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_model(12920, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer(min_df = 0)\n",
    "tf_idf_feature = tf_idf_vectorizer.fit_transform(data['title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_model(doc_id, num_results):\n",
    "    pair_wise_dist = pairwise_distances(tf_idf_feature, tf_idf_feature[doc_id])\n",
    "\n",
    "    #indices of smalles distances\n",
    "    indices = np.argsort(pair_wise_dist.flatten())[0:num_results]\n",
    "\n",
    "    # smalles distances\n",
    "    pdists = np.sort(pair_wise_dist.flatten())[0:num_results]\n",
    "\n",
    "    df_indices = list(data.index[indices])\n",
    "\n",
    "    for i in range(0, len(indices)):\n",
    "        get_result(indices[i],data['title'].loc[df_indices[0]], data['title'].loc[df_indices[i]], data['medium_image_url'].loc[df_indices[i]], 'bag_of_words')\n",
    "\n",
    "        print('ASIN:', data['asin'].loc[df_indices[i]])\n",
    "        print('Brand:', data['brand'].loc[df_indices[i]])\n",
    "        print('Title:', data['title'].loc[df_indices[i]])\n",
    "        print('Euclidean similarity with the query image :', pdists[i])\n",
    "        print('*'*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model(12920,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Weighted Word2Vec or Text semantic based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_title_vectorizer = CountVectorizer()\n",
    "idf_title_features = idf_title_vectorizer.fit_transform(data['title'])\n",
    "\n",
    "def n_containing(word):\n",
    "    # return the number of documents which had the given word\n",
    "    return sum(1 for blob in data['title'] if word in blob.split())\n",
    "\n",
    "def idf(word):\n",
    "    # idf = log(#number of docs / #number of docs which had the given word)\n",
    "    return math.log(data.shape[0] / (n_containing(word)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to convert the values into float\n",
    "idf_title_features  = idf_title_features.astype(np.float)\n",
    "\n",
    "for i in idf_title_vectorizer.vocabulary_.keys():\n",
    "    # for every word in whole corpus we will find its idf value\n",
    "    idf_val = idf(i)\n",
    "    \n",
    "    # to calculate idf_title_features we need to replace the count values with the idf values of the word\n",
    "    # idf_title_features[:, idf_title_vectorizer.vocabulary_[i]].nonzero()[0] will return all documents in which the word i present\n",
    "    for j in idf_title_features[:, idf_title_vectorizer.vocabulary_[i]].nonzero()[0]:\n",
    "        \n",
    "        # we replace the count values of word i in document j with  idf_value of word i \n",
    "        # idf_title_features[doc_id, index_of_word_in_courpus] = idf value of word\n",
    "        idf_title_features[j,idf_title_vectorizer.vocabulary_[i]] = idf_val\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vec(sentence, doc_id , m_name):\n",
    "    #sentence = title, doc_id = id\n",
    "    vec = []\n",
    "    for i in sentence.split():\n",
    "        if i in vocab:\n",
    "            if m_name == 'weighted' and i in idf_title_vectorizer.vocabulary_:\n",
    "                vec.append(idf_title_features[doc_id, idf_title_vectorizer.vocabulary_[i]] * model[i])\n",
    "            elif m_name == 'avg':\n",
    "                vec.append(model[i])\n",
    "        else:\n",
    "            vec.append(np,zeros(shape = (300,)))\n",
    "    return np.array(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(vec1, vec2):\n",
    "    final_dist = []\n",
    "    \n",
    "    # for each vector in vec1 we calculate the distance(euclidean) to all vectors in vec2\n",
    "    for i in vec1:\n",
    "        dist = []\n",
    "        for j in vec2:\n",
    "            dist.append(np.linalg.norm(i-j))\n",
    "        final_dist.append(np.array(dist))\n",
    "        \n",
    "    return np.array(final_dist)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heat_map_w2v(sentence1, sentence2, url, doc_id1, doc_id2, model):\n",
    "    # sentance1 : title1, input apparel\n",
    "    # sentance2 : title2, recommended apparel\n",
    "    # url: apparel image url\n",
    "    # doc_id1: document id of input apparel\n",
    "    # doc_id2: document id of recommended apparel\n",
    "    # model: it can have two values, 1. avg 2. weighted\n",
    "    \n",
    "    #s1_vec = np.array(#number_of_words_title1 * 300), each row is a vector(weighted/avg) of length 300 corresponds to each word in give title\n",
    "    s1_vec = get_word_vec(sentence1, doc_id1, model)\n",
    "    #s2_vec = np.array(#number_of_words_title1 * 300), each row is a vector(weighted/avg) of length 300 corresponds to each word in give title\n",
    "    s2_vec = get_word_vec(sentence2, doc_id2, model)\n",
    "\n",
    "    # s1_s2_dist = np.array(#number of words in title1 * #number of words in title2)\n",
    "    # s1_s2_dist[i,j] = euclidean distance between words i, j\n",
    "    s1_s2_dist = get_distance(s1_vec, s2_vec)\n",
    "\n",
    "    \n",
    "    \n",
    "    # devide whole figure into 2 parts 1st part displays heatmap 2nd part displays image of apparel\n",
    "    gs = gridspec.GridSpec(2, 2, width_ratios=[4,1],height_ratios=[2,1]) \n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    \n",
    "    ax = plt.subplot(gs[0])\n",
    "    # ploting the heap map based on the pairwise distances\n",
    "    ax = sns.heatmap(np.round(s1_s2_dist,4), annot=True)\n",
    "    # set the x axis labels as recommended apparels title\n",
    "    ax.set_xticklabels(sentence2.split())\n",
    "    # set the y axis labels as input apparels title\n",
    "    ax.set_yticklabels(sentence1.split())\n",
    "    # set title as recommended apparels title\n",
    "    ax.set_title(sentence2)\n",
    "    \n",
    "    ax = plt.subplot(gs[1])\n",
    "    # we remove all grids and axis labels for image\n",
    "    ax.grid(False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    display_img(url, ax, fig)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word2vec_model', 'rb') as handle:\n",
    "    model = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = model.keys()\n",
    "\n",
    "# this function witll add the vectors fo each word and returns the avg vector of given sentance\n",
    "def build_avg_vec(sentence, num_features, doc_id, m_name):\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    # we will intialize a vector of size 300 with all zeros\n",
    "    # we add each word2vec(wordi) to this fetureVec\n",
    "    nwords = 0\n",
    "    \n",
    "    for word in sentence.split():\n",
    "        nwords += 1\n",
    "        if word in vocab:\n",
    "            if m_name == 'weighted' and word in  idf_title_vectorizer.vocabulary_:\n",
    "                featureVec = np.add(featureVec, idf_title_features[doc_id, idf_title_vectorizer.vocabulary_[word]] * model[word])\n",
    "            elif m_name == 'avg':\n",
    "                featureVec = np.add(featureVec, model[word])\n",
    "    if(nwords>0):\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    # returns the avg vector of given sentance, its of shape (1, 300)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = 0\n",
    "w2v_title = []\n",
    "# for every title we build a avg vector representation\n",
    "for i in data['title']:\n",
    "    w2v_title.append(build_avg_vec(i, 300, doc_id,'avg'))\n",
    "    doc_id += 1\n",
    "\n",
    "# w2v_title = np.array(# number of doc in courpus * 300), each row corresponds to a doc \n",
    "w2v_title = np.array(w2v_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_w2v_model(doc_id, num_results):\n",
    "\n",
    "    pairwise_dist = pairwise_distances(w2v_title, w2v_title[doc_id].reshape(1,-1))\n",
    "\n",
    "    # np.argsort will return indices of 9 smallest distances\n",
    "    indices = np.argsort(pairwise_dist.flatten())[0:num_results]\n",
    "    #pdists will store the 9 smallest distances\n",
    "    pdists  = np.sort(pairwise_dist.flatten())[0:num_results]\n",
    "\n",
    "    #data frame indices of the 9 smallest distace's\n",
    "    df_indices = list(data.index[indices])\n",
    "    \n",
    "    for i in range(0, len(indices)):\n",
    "        heat_map_w2v(data['title'].loc[df_indices[0]],data['title'].loc[df_indices[i]], data['medium_image_url'].loc[df_indices[i]], indices[0], indices[i], 'avg')\n",
    "        print('ASIN :',data['asin'].loc[df_indices[i]])\n",
    "        print('BRAND :',data['brand'].loc[df_indices[i]])\n",
    "        print ('euclidean distance from given input image :', pdists[i])\n",
    "        print('*'*125)\n",
    "\n",
    "        \n",
    "avg_w2v_model(12920, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. IDF weighted Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = 0\n",
    "w2v_title_weight = []\n",
    "# for every title we build a weighted vector representation\n",
    "for i in data['title']:\n",
    "    w2v_title_weight.append(build_avg_vec(i, 300, doc_id,'weighted'))\n",
    "    doc_id += 1\n",
    "# w2v_title = np.array(# number of doc in courpus * 300), each row corresponds to a doc \n",
    "w2v_title_weight = np.array(w2v_title_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_w2v_model(doc_id, num_results):\n",
    "    \n",
    "    pairwise_dist = pairwise_distances(w2v_title_weight, w2v_title_weight[doc_id].reshape(1,-1))\n",
    "\n",
    "    # np.argsort will return indices of 9 smallest distances\n",
    "    indices = np.argsort(pairwise_dist.flatten())[0:num_results]\n",
    "    #pdists will store the 9 smallest distances\n",
    "    pdists  = np.sort(pairwise_dist.flatten())[0:num_results]\n",
    "\n",
    "    #data frame indices of the 9 smallest distace's\n",
    "    df_indices = list(data.index[indices])\n",
    "    \n",
    "    for i in range(0, len(indices)):\n",
    "        heat_map_w2v(data['title'].loc[df_indices[0]],data['title'].loc[df_indices[i]], data['medium_image_url'].loc[df_indices[i]], indices[0], indices[i], 'weighted')\n",
    "        print('ASIN :',data['asin'].loc[df_indices[i]])\n",
    "        print('Brand :',data['brand'].loc[df_indices[i]])\n",
    "        print('euclidean distance from input :', pdists[i])\n",
    "        print('='*125)\n",
    "\n",
    "weighted_w2v_model(12920, 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "A_RS",
   "language": "python",
   "name": "a_rs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
